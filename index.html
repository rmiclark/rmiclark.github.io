

<!DOCTYPE html>
<html>
    <head>
        <!-- Thanks to Antoine Bosselut for the website template -->
        <meta content="text/html; charset=UTF-8" http-equiv="Content-Type"/>
        <title>Dallas Card</title>
        <meta content="Antoine Bosselut" name="Author"/>
        <link href="style.css" type="text/css" rel="stylesheet"/>
        <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">
        <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>
        <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy" crossorigin="anonymous"></script>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-EL45TK68M9"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-EL45TK68M9');
    </script>
    </head>
    <body>
        <div class="page">
            <div class="container">
                <div class="row">
                <!-- <div class="col-md-1">&nbsp;</div> -->
                    <div class="col-md-4">
                        <img alt="Mike Clark" src="IMG_8413.jpg" width="95%" style="float: center;"/>
                    </div>
                <!-- <div class="col-md-1">&nbsp;</div> -->
                    <div class="col-md-8">
                        <br>
                            <h1 id="name"> Mike Clark </h1>
                            Monetization Strategy, <a target="_blank" href="https://www.linkedin.com/in/miclark/">LinkedIn</a><br>
                            <br>
                            Email: rmiclark@live.edu<br>
                            <a target="_blank" href="https://www.github.com/dallascard">GitHub</a>, <a target="_blank" href="https://www.twitter.com/dallascard">Twitter</a>, <a target="_blank" href="https://dallascard.github.io/granular-material">Blog</a>
                            <br>
                            <a target="_blank" href="https://scholar.google.com/citations?hl=en&user=qH-rJV8AAAAJ&view_op=list_works&sortby=pubdate">Google Scholar</a>, <a target="_blank" href="https://orcid.org/0000-0001-5573-8836">ORCiD</a>
                            <br>
                            <br>                                
                                I am an assistant professor in the <a target="_blank" href="https://www.si.umich.edu/">School of Information</a> at the University of Michigan. Before that, I was a postdoctoral researcher in the <a target="_blank" href="https://nlp.stanford.edu/">Stanford NLP Group</a> and the <a target="_blank" href="https://datascience.stanford.edu/people/data-science-scholars/grid">Stanford Data Science Institute</a>. 
                                I received my Ph.D. from the <a target="_blank" href="http://www.ml.cmu.edu/">Machine Learning Department</a> at Carnegie Mellon University, where I was advised by <a target="_blank" href="https://homes.cs.washington.edu/~nasmith/">Noah Smith</a>.
                                <br>
                                <br>                                            
                                My research centers on making machine learning more reliable and responsible, and on using machine learning and natural language processing to learn about society from text. 
                    </div>
                </div>
                <br>
                <br>
                <div class="row">
                    <h2> Updates </h2>
                </div>
                <br>
                <div class="row news-container">
                    <ul>
                        <li>November 2022: Our <a target="_blank" href="https://arxiv.org/abs/2201.10474">paper</a> on language ideology and training data selection for language models was accepted at EMNLP!</li> 
                        <li>November 2022: I will be presenting at the Georgia Tech NLP seminar on November 4, 2022.</li> 
                        <li>October 2022: Our <a target="_blank" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4200618">paper</a> on media coverage of mass shootings has now been published in Political Communication (tweet summary <a target="_blank" href="https://twitter.com/dallascard/status/1580226173006909441">here</a>).</li> 
                        <li>October 2022: I will be attending <a target="_blank" href="https://tada2022.org/">Text as Data</a> at Cornell Tech in New York City, October 6-7, 2022.</li> 
                        <li>September 2022: New <a target="_blank" href="https://dallascard.github.io/granular-material/post/remote-work-bot/">blog post</a> about the recent manipulation of <a target="_blank" href="https://twitter.com/remoteli_io">@remoteli_io</a>.</li>                                
                        <li>August 2022: Our paper on the history of immigration discourse in congress is now out in <a target="_blank" href="https://www.pnas.org/doi/10.1073/pnas.2120510119">PNAS</a>!</li>
                        <li>July 2022: I will be at NAACL in Seattle, July 11-15th.</li>
                        <li>June 2022: Very excited to have received a distinguished paper award at FAccT 2022 for our <a target="_blank" href="https://facctconference.org/static/pdfs_2022/facct22-14.pdf">paper</a> on the Values in Machine Learning!! </li>
                    </ul>
                </div>
                <br>

                <div class="row">
                    <h2> Selected Publications</h2>
                </div>
                <br>


                <div class="row">
                <!-- <div class="col-md-1">&nbsp;</div> -->
                    <div class="col-md-4">        
                        <img alt="Correlation between GPT-3 quality sores and demographic factors" src="resources/quality_scatters.png" width="80%" style="float: center;"/>
                    </div>
                    <!-- <div class="col-md-1">&nbsp;</div> -->
                    <div class="col-md-8"  style="background-color: #DCDCDC; margin: auto;">
                        <br><a target="_blank" href="https://arxiv.org/abs/2201.10474"><b>Whose Language Counts as High Quality? Measuring Language Ideologies in Text Data Selection</b></a> <br>
                        Suchin Gururangan, <u>Dallas Card</u>, Sarah K. Dreier, Emily K. Gade, Leroy Z. Wang, Zeyu Wang, Luke Zettlemoyer, Noah A. Smith<br>
                        In <i>Proceedings of EMNLP</i>, 2022.<br>
                        <a data-toggle="collapse" href="#quality-abstract" class="btn-sm btn-primary">Abstract</a>
                        <a target="_blank" href="https://arxiv.org/pdf/2201.10474.pdf" class="btn-sm btn-primary">Paper</a>
                        <a target="_blank" href="https://github.com/kernelmachine/quality-filter" class="btn-sm btn-primary">Data and Code</a>                        
                        <a target="_blank" href="bibtex/emnlp2022-bibtex.txt" class="btn-sm btn-primary">BibTeX</a>
                        <br><br>
                        <div id="quality-abstract" class="collapse" style="height: auto;">
                            Language models increasingly rely on massive web dumps for diverse text data. However, these sources are rife with undesirable content. As such, resources like Wikipedia, books, and newswire often serve as anchors for automatically selecting web text most suitable for language modeling, a process typically referred to as quality filtering. Using a new dataset of U.S. high school newspaper articles -- written by students from across the country -- we investigate whose language is preferred by the quality filter used for GPT-3. We find that newspapers from larger schools, located in wealthier, educated, and urban ZIP codes are more likely to be classified as high quality. We then demonstrate that the filter's measurement of quality is unaligned with other sensible metrics, such as factuality or literary acclaim. We argue that privileging any corpus as high quality entails a language ideology, and more care is needed to construct training corpora for language models, with better transparency and justification for the inclusion or exclusion of various texts.
                            <br><br>
                        </div>
                    </div>
                </div>
                <br>                
                <br>    

                <div class="row">
                <!-- <div class="col-md-1">&nbsp;</div> -->
                    <div class="col-md-4">        
                        <img alt="Net tone of immigration speeches over time" src="resources/immigration_tone.png" width="80%" style="float: center;"/>
                    </div>
                    <!-- <div class="col-md-1">&nbsp;</div> -->
                    <div class="col-md-8"  style="background-color: #DCDCDC; margin: auto;">
                        <br><a target="_blank" href="https://www.pnas.org/doi/10.1073/pnas.2120510119"><b>Computational analysis of 140 years of US political speeches reveals more positive but increasingly polarized framing of immigration</b></a> <br>
                        <u>Dallas Card</u>, Serina Chang, Chris Becker, Julia Mendelsohn, Rob Voigt, Leah Boustan, Ran Abramitzky, Dan Jurafsky<br>
                        In <i>Proceedings of the National Academy of Sciences 119(31)</i>, 2022.<br>
                        <a data-toggle="collapse" href="#immigration-abstract" class="btn-sm btn-primary">Abstract</a>
                        <a target="_blank" href="resources/pnas.2022.pdf" class="btn-sm btn-primary">Paper</a>
                        <a target="_blank" href="https://github.com/dallascard/us-immigration-speeches/" class="btn-sm btn-primary">Data and Code</a>                        
                        <a target="_blank" href="bibtex/pnas2022-bibtex.txt" class="btn-sm btn-primary">BibTeX</a>
                        <br><br>
                        <div id="immigration-abstract" class="collapse" style="height: auto;">
                            We classify and analyze 200,000 US congressional speeches and 5,000 presidential communications related to immigration from 1880 to the present. Despite the salience of antiimmigration rhetoric today, we find that political speech about immigration is now much more positive on average than in the past, with the shift largely taking place between World War II and the passage of the Immigration and Nationality Act in 1965. However, since the late 1970s, political parties have become increasingly polarized in their expressed attitudes toward immigration, such that Republican speeches today are as negative as the average congressional speech was in the 1920s, an era of strict immigration quotas. Using an approach based on contextual embeddings of text, we find that modern Republicans are significantly more likely to use language that is suggestive of metaphors long associated with immigration, such as "animals" and "cargo," and make greater use of frames like "crime" and "legality." The tone of speeches also differs strongly based on which nationalities are mentioned, with a striking similarity between how Mexican immigrants are framed today and how Chinese immigrants were framed during the era of Chinese exclusion in the late 19th century. Overall, despite more favorable attitudes toward immigrants and the formal elimination of race-based restrictions, nationality is still a major factor in how immigrants are spoken of in Congress.
                            <br><br>
                        </div>
                    </div>
                </div>
                <br>                
                <br>    


                <div class="row">
                <!-- <div class="col-md-1">&nbsp;</div> -->
                    <div class="col-md-4">        
                        <img alt="The Values Encoded in Machine Learning Research" src="resources/values-figure.png" width="80%" style="float: center;"/>
                    </div>
                    <!-- <div class="col-md-1">&nbsp;</div> -->
                    <div class="col-md-8"  style="background-color: #DCDCDC; margin: auto;">
                        <br><a target="_blank" href="https://arxiv.org/abs/2106.15590"><b>The Values Encoded in Machine Learning Research</b></a> [Distinguished Paper Award]<br>
                        Abeba Birhane, Pratyusha Kalluri, <u>Dallas Card</u>, William Agnew, Ravit Dotan, and Michelle Bao<br>
                        In <i>Proceedings of FAccT</i>, 2022.<br>
                        <a data-toggle="collapse" href="#values-abstract" class="btn-sm btn-primary">Abstract</a>
                        <a target="_blank" href="https://arxiv.org/abs/2106.15590" class="btn-sm btn-primary">Paper</a>
                        <a target="_blank" href="https://github.com/wagnew3/The-Values-Encoded-in-Machine-Learning-Research" class="btn-sm btn-primary">Data and Code</a>                        
                        <a target="_blank" href="bibtex/values-bibtex.txt" class="btn-sm btn-primary">BibTeX</a>
                        <br><br>
                        <div id="values-abstract" class="collapse" style="height: auto;">
                            Machine learning currently exerts an outsized influence on the world, increasingly affecting institutional practices and impactedcommunities. It is therefore critical that we question vague conceptions of the field as value-neutral or universally beneficial, and investigate what specific values the field is advancing. In this paper, we first introduce a method and annotation scheme for studying the values encoded in documents such as research papers. Applying the scheme, we analyze 100 highly cited machine learning papers published at premier machine learning conferences, ICML and NeurIPS. We annotate key features of papers which reveal their values: their justification for their choice of project, which attributes of their project they uplift, their consideration of potential negative consequences, and their institutional affiliations and funding sources. We find that few of the papers justify how their project connects to a societal need (15%) and far fewer discuss negative potential (1%). Through line-by-line content analysis, we identify 59 values that are uplifted in ML research, and, of these, we find that the papers most frequently justify and assess themselves based on Performance, Generalization, Quantitative evidence, Efficiency, Building on past work, and Novelty. We present extensive textual evidence and identify key themes in the definitions and operationalization of these values. Notably, we find systematic textual evidence that these top values are being defined and applied with assumptions and implications generally supporting the centralization of power. Finally, we find increasingly close ties between these highly cited papers and tech companies and elite universities.
                            <br><br>
                        </div>
                    </div>
                </div>
                <br>                
                <br>    


                <div class="row">
                <!-- <div class="col-md-1">&nbsp;</div> -->
                    <div class="col-md-4">
                        <img alt="Modular Domain Adaptation" src="resources/chen.2022-diagram.png" width="70%" style="float: center;"/>
                    </div>
                <!-- <div class="col-md-1">&nbsp;</div> -->
                    <div class="col-md-8"  style="background-color: #DCDCDC; margin: auto;">
                        <br><a target="_blank" href="https://aclanthology.org/2022.findings-acl.288/"><b>Modular Domain Adaptation</b></a><br>
                        Junshen Chen, <u>Dallas Card</u>, Dan Jurafsky<br>
                        In <i>Findings of ACL</i>, 2022.<br>
                        <a data-toggle="collapse" href="#modular-abstract" class="btn-sm btn-primary">Abstract</a>
                        <a target="_blank" href="https://aclanthology.org/2022.findings-acl.288.pdf" class="btn-sm btn-primary">Paper</a>
                        <a target="_blank" href="https://github.com/jkvc/modular-domain-adaptation" class="btn-sm btn-primary">Code</a>
                        <a target="_blank" href="https://granularmaterial.com/modular-domain-adaptation/"  class="btn-sm btn-primary">Blog Post</a>
                        <a target="_blank" href="bibtex/acl2022a-bibtex.txt" class="btn-sm btn-primary">BibTeX</a>
                        <br><br>
                        <div id="modular-abstract" class="collapse" style="height: auto;">
                             Off-the-shelf models are widely used by computational social science researchers to measure properties of text, such as sentiment.However, without access to source data it is difficult to account for domain shift, which represents a threat to validity. Here, we treat domain adaptation as a modular process that involves separate model producers and model consumers, and show how they can independently cooperate to facilitate more accurate measurements of text. We introduce two lightweight techniques for this scenario, and demonstrate that they reliably increase out-of-domain accuracy on four multi-domain text classification datasets when used with linear and contextual embedding models. We conclude with recommendations for model producers and consumers, and release models and replication code to accompany this paper.
                            <br><br>
                        </div>
                    </div>
                </div>
                <br>                
                <br>   

                <div class="row">
                <!-- <div class="col-md-1">&nbsp;</div> -->
                    <div class="col-md-4">
                        <img alt="Problems with Cosine" src="resources/acl2022-figure.png"  width="80%" style="float: center;"/> 
                    </div>
                <!-- <div class="col-md-1">&nbsp;</div> -->
                    <div class="col-md-8"  style="background-color: #DCDCDC; margin: auto;">
                        <br><a target="_blank" href="https://aclanthology.org/2022.acl-short.45/"><b>Problems with Cosine as a Measure of Embedding Similarity for High Frequency Words</b></a><br>
                        Kaitlyn Zhou, Kawin Ethayarajh, <u>Dallas Card</u>, Dan Jurafsky<br>
                        In <i>Proceedings of ACL</i>, 2022.<br>
                        <a data-toggle="collapse" href="#acl2022-abstract" class="btn-sm btn-primary">Abstract</a>
                        <a target="_blank" href="https://aclanthology.org/2022.acl-short.45.pdf" class="btn-sm btn-primary">Paper</a>
                        <a target="_blank" href="bibtex/acl2022b-bibtex.txt" class="btn-sm btn-primary">BibTeX</a>
                        <br><br>
                        <div id="acl2022-abstract" class="collapse" style="height: auto;">
                             Cosine similarity of contextual embeddings is used in many NLP tasks (e.g., QA, IR, MT) and metrics (e.g., BERTScore). Here, we uncover systematic ways in which word similarities estimated by cosine over BERT embeddings are understated and trace this effect to training data frequency. We find that relative to human judgements, cosine similarity underestimates the similarity of frequent words with other instances of the same word or other words across contexts, even after controlling for polysemy and other factors. We conjecture that this underestimation of similarity for high frequency words is due to differences in the representational geometry of high and low frequency words and provide a formal argument for the two-dimensional case.  
                            <br><br>
                        </div>
                    </div>
                </div>
                <br>                
                <br>    


                <div class="row">
                <!-- <div class="col-md-1">&nbsp;</div> -->
                    <div class="col-md-4">
                        <img alt="On the Opportunities and Risks of Foundation Models" src="resources/foundation.png" width="70%" style="float: center;"/>
                    </div>
                <!-- <div class="col-md-1">&nbsp;</div> -->
                    <div class="col-md-8"  style="background-color: #DCDCDC; margin: auto;">
                        <br><a target="_blank" href="https://arxiv.org/abs/2108.07258"><b>On the Opportunities and Risks of Foundation Models</b></a><br>
                        Rishi Bommasani, Drew A. Hudson, Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, <u>Dallas Card</u>, et al.<br>
                        <i>arXiv:2108.07258</i>, 2021.<br>
                        <a data-toggle="collapse" href="#foundation-abstract" class="btn-sm btn-primary">Abstract</a>
                        <a target="_blank" href="https://arxiv.org/pdf/2108.07258.pdf" class="btn-sm btn-primary">Paper</a>
                        <a target="_blank" href="bibtex/foundation-bibtex.txt" class="btn-sm btn-primary">BibTeX</a>
                        <br><br>
                        <div id="foundation-abstract" class="collapse" style="height: auto;">
                             AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.  
                            <br><br>
                        </div>
                    </div>
                </div>
                <br>                
                <br>    


                <div class="row">
                <!-- <div class="col-md-1">&nbsp;</div> -->
                    <div class="col-md-4">
                        <img alt="Expected Validation Performance and Estimation of a Random Variable's Maximum" src="resources/emnlp2021.png" width="70%" style="float: center;"/>
                    </div>
                <!-- <div class="col-md-1">&nbsp;</div> -->
                    <div class="col-md-8"  style="background-color: #DCDCDC; margin: auto;">
                        <br><a target="_blank" href="https://arxiv.org/abs/2110.00613"><b>Expected Validation Performance and Estimation of a Random Variable's Maximum</b></a><br>
                        Jesse Dodge, Suchin Gururangan, <u>Dallas Card</u>, Roy Schwartz, Noah A. Smith<br>
                        In <i>Findings of EMNLP</i>, 2021.<br>
                        <a data-toggle="collapse" href="#emnlp2021-abstract" class="btn-sm btn-primary">Abstract</a>
                        <a target="_blank" href="https://arxiv.org/pdf/2110.00613.pdf" class="btn-sm btn-primary">Paper</a>
                        <a target="_blank" href="bibtex/emnlp2021-bibtex.txt" class="btn-sm btn-primary">BibTeX</a>
                        <br><br>
                        <div id="emnlp2021-abstract" class="collapse" style="height: auto;">
                             Research in NLP is often supported by experimental results, and improved reporting of such results can lead to better understanding and more reproducible science. In this paper we analyze three statistical estimators for expected validation performance, a tool used for reporting performance (e.g., accuracy) as a function of computational budget (e.g., number of hyperparameter tuning experiments). Where previous work analyzing such estimators focused on the bias, we also examine the variance and mean squared error (MSE). In both synthetic and realistic scenarios, we evaluate three estimators and find the unbiased estimator has the highest variance, and the estimator with the smallest variance has the largest bias; the estimator with the smallest MSE strikes a balance between bias and variance, displaying a classic bias-variance tradeoff. We use expected validation performance to compare between different models, and analyze how frequently each estimator leads to drawing incorrect conclusions about which of two models performs best. We find that the two biased estimators lead to the fewest incorrect conclusions, which hints at the importance of minimizing variance and MSE.   
                            <br><br>
                        </div>
                    </div>
                </div>
                <br>                
                <br>                    

                <div class="row">
                <!-- <div class="col-md-1">&nbsp;</div> -->
                    <div class="col-md-4">
                        <img alt="Causal Effects of Linguistic Properties" src="resources/text-causal.png" width="80%" style="float: center;"/>
                    </div>
                <!-- <div class="col-md-1">&nbsp;</div> -->
                    <div class="col-md-8"  style="background-color: #DCDCDC; margin: auto;">
                        <br><a target="_blank" href="https://arxiv.org/abs/2010.12919"><b>Causal Effects of Linguistic Properties</b></a><br>
                        Reid Pryzant, <u>Dallas Card</u>, Dan Jurafsky, Victor Veitch, and Dhanya Sridhar<br>
                        In <i>Proceedings of NAACL</i>, 2021.<br>
                        <a data-toggle="collapse" href="#naacl2021-abstract" class="btn-sm btn-primary">Abstract</a>
                        <a target="_blank" href="https://arxiv.org/pdf/2010.12919.pdf" class="btn-sm btn-primary">Paper</a>
                        <a target="_blank" href="bibtex/naacl2021-bibtex.txt" class="btn-sm btn-primary">BibTeX</a>
                        <br><br>
                        <div id="naacl2021-abstract" class="collapse" style="height: auto;">
                            We consider the problem of using observational data to estimate the causal effects of linguistic properties. For example, does writing a complaint politely lead to a faster response time? How much will a positive product review increase sales? This paper addresses two technical challenges related to the problem before developing a practical method. First, we formalize the causal quantity of interest as the effect of a writer's intent, and establish the assumptions necessary to identify this from observational data. Second, in practice, we only have access to noisy proxies for the linguistic properties of interest -- e.g., predictions from classifiers and lexicons. We propose an estimator for this setting and prove that its bias is bounded when we perform an adjustment for the text. Based on these results, we introduce TextCause, an algorithm for estimating causal effects of linguistic properties. The method leverages (1) distant supervision to improve the quality of noisy proxies, and (2) a pre-trained language model (BERT) to adjust for the text. We show that the proposed method outperforms related approaches when estimating the effect of Amazon review sentiment on semi-simulated sales figures. Finally, we present an applied case study investigating the effects of complaint politeness on bureaucratic response times.  
                            <br><br>
                        </div>
                    </div>
                </div>
                <br>                
                <br>    

                <div class="row">
                <!-- <div class="col-md-1">&nbsp;</div> -->
                    <div class="col-md-4">
                        <img alt="With Little Power Comes Great Responsibility" src="resources/humaneval_highlow.png" width="80%" style="float: center;"/>
                    </div>
                <!-- <div class="col-md-1">&nbsp;</div> -->
                    <div class="col-md-8"  style="background-color: #DCDCDC; margin: auto;">
                        <br><a target="_blank" href="https://arxiv.org/abs/2010.06595"><b>With Little Power Comes Great Responsibility</b></a><br>
                        <u>Dallas Card</u>, Peter Henderson, Urvashi Khandelwal, Robin Jia, Kyle Mahowald, and Dan Jurafsky<br>
                        In <i>Proceedings of EMNLP</i>, 2020.<br>
                        <a data-toggle="collapse" href="#power-abstract" class="btn-sm btn-primary">Abstract</a>
                        <a target="_blank" href="https://arxiv.org/pdf/2010.06595.pdf" class="btn-sm btn-primary">Paper</a>
                        <a target="_blank" href="https://github.com/dallascard/NLP-power-analysis" class="btn-sm btn-primary">Code</a>
                        <a target="_blank" href="bibtex/power-bibtex.txt" class="btn-sm btn-primary">BibTeX</a>                                                
                        <br><br>
                        <div id="power-abstract" class="collapse" style="height: auto;">
                            Despite its importance to experimental design, statistical power (the probability that, given a real effect, an experiment will reject the null hypothesis) has largely been ignored by the NLP community. Underpowered experiments make it more difficult to discern the difference between statistical noise and meaningful model improvements, and increase the chances of exaggerated findings. By meta-analyzing a set of existing NLP papers and datasets, we characterize typical power for a variety of settings and conclude that underpowered experiments are common in the NLP literature. In particular, for several tasks in the popular GLUE benchmark, small test sets mean that most attempted comparisons to state of the art models will not be adequately powered. Similarly, based on reasonable assumptions, we find that the most typical experimental design for human rating studies will be underpowered to detect small model differences, of the sort that are frequently studied. For machine translation, we find that typical test sets of 2000 sentences have approximately 75% power to detect differences of 1 BLEU point. To improve the situation going forward, we give an overview of best practices for power analysis in NLP and release a series of notebooks to assist with future power analyses. 
                            <br><br>
                        </div>
                    </div>
                </div>
                <br>                
                <br>    

                <div class="row">
                <!-- <div class="col-md-1">&nbsp;</div> -->
                    <div class="col-md-4">
                        <img alt="Detecting Stance in Media On Global Warming" src="resources/GWSD.png" width="80%" style="float: center;"/>
                    </div>
                <!-- <div class="col-md-1">&nbsp;</div> -->
                    <div class="col-md-8"  style="background-color: #DCDCDC; margin: auto;">
                        <br><a target="_blank" href="https://arxiv.org/abs/2010.15149"><b>Detecting Stance in Media On Global Warming</b></a><br>
                        Yiwei Luo, <u>Dallas Card</u>, and Dan Jurafsky<br>
                        In <i>Findings of EMNLP</i>, 2020.<br>
                        <a data-toggle="collapse" href="#GWSD-abstract" class="btn-sm btn-primary">Abstract</a>
                        <a target="_blank" href="https://arxiv.org/pdf/2010.15149.pdf" class="btn-sm btn-primary">Paper</a>
                        <a target="_blank" href="https://github.com/yiweiluo/GWstance" class="btn-sm btn-primary">Code</a>
                        <a target="_blank" href="bibtex/GWSD-bibtex.txt" class="btn-sm btn-primary">BibTeX</a>                                                
                        <br><br>
                        <div id="GWSD-abstract" class="collapse" style="height: auto;">
                            Citing opinions is a powerful yet understudied strategy in argumentation. For example, an environmental activist might say, "Leading scientists agree that global warming is a serious concern," framing a clause which affirms their own stance ("that global warming is serious") as an opinion endorsed ("[scientists] agree") by a reputable source ("leading"). In contrast, a global warming denier might frame the same clause as the opinion of an untrustworthy source with a predicate connoting doubt: "Mistaken scientists claim [...]." Our work studies opinion-framing in the global warming (GW) debate, an increasingly partisan issue that has received little attention in NLP. We introduce Global Warming Stance Dataset (GWSD), a dataset of stance-labeled GW sentences, and train a BERT classifier to study novel aspects of argumentation in how different sides of a debate represent their own and each other's opinions. From 56K news articles, we find that similar linguistic devices for self-affirming and opponent-doubting discourse are used across GW-accepting and skeptic media, though GW-skeptical media shows more opponent-doubt. We also find that authors often characterize sources as hypocritical, by ascribing opinions expressing the author's own view to source entities known to publicly endorse the opposing view. We release our stance dataset, model, and lexicons of framing devices for future work on opinion-framing and the automatic detection of GW stance.  
                            <br><br>
                        </div>
                    </div>
                </div>
                <br>                
                <br>                

                <div class="row">
                <!-- <div class="col-md-1">&nbsp;</div> -->
                    <div class="col-md-4">
                        <img alt="Explain like I am a Scientist: The Linguistic Barriers of Entry to r/science" src="resources/rscience.png" width="80%" style="float: center;"/>
                    </div>
                <!-- <div class="col-md-1">&nbsp;</div> -->
                    <div class="col-md-8"  style="background-color: #DCDCDC; margin: auto;">
                        <br><a target="_blank" href="resources/CHI2020.pdf"><b>Explain like I am a Scientist: The Linguistic Barriers of Entry to r/science</b></a><br>
                        Tal August, <u>Dallas Card</u>, Gary Hsieh, Noah A. Smith, and Katharina Reinecke<br>
                        In <i>Human Factors in Computing Systems (CHI)</i>, 2020.<br>
                        <a data-toggle="collapse" href="#rscience-abstract" class="btn-sm btn-primary">Abstract</a>
                        <a target="_blank" href="resources/CHI2020.pdf" class="btn-sm btn-primary">Paper</a>
                        <a target="_blank" href="bibtex/CHI2020-bibtex.txt" class="btn-sm btn-primary">BibTeX</a>                                                
                        <br><br>
                        <div id="rscience-abstract" class="collapse" style="height: auto;">
                            As an online community for discussing research findings, <i>r/science</i> has the potential to contribute to science outreach and communication with a broad audience. Yet previous work suggests that most of the active contributors on <i>r/science</i> are science-educated  people rather than a lay general public. One potential reason is that <i>r/science</i> contributors might use a different, more specialized language than used in other subreddits. To investigate this possibility, we analyzed the language used in more than 68 million  posts and comments from 12 subreddits from 2018. We show that <i>r/science</i> uses a specialized language that is distinct from other subreddits. Transient (newer) authors of posts and comments on <i>r/science</i> use less specialized language than more frequent authors, and those that leave the community use less specialized language than those that stay, even when comparing their first comments. These findings suggest that the specialized language used in <i>r/science</i> has a gatekeeping effect, preventing participation by people whose language does not align with that used in <i>r/science</i>. By characterizing <i>r/science</i>'s specialized language, we contribute guidelines and tools for increasing the number of contributors in <i>r/science</i>. 
                            <br><br>
                        </div>
                    </div>
                </div>
                <br>                
                <br>           


                <div class="row">
                <!-- <div class="col-md-1">&nbsp;</div> -->
                    <div class="col-md-4">
                    </div>
                <!-- <div class="col-md-1">&nbsp;</div> -->
                    <div class="col-md-8"  style="background-color: #DCDCDC; margin: auto;">
                        <br><a target="_blank" href="https://arxiv.org/abs/2001.00329"><b>On Consequentialism and Fairness</b></a><br>
                        <u>Dallas Card</u> and Noah A. Smith<br>
                        <i>Frontiers in Artificial Intelligence</i>, 2020.<br>
                        <a data-toggle="collapse" href="#fairness-abstract" class="btn-sm btn-primary">Abstract</a>
                        <a target="_blank" href="https://arxiv.org/pdf/2001.00329.pdf" class="btn-sm btn-primary">Paper</a>
                        <a target="_blank" href="bibtex/fairness-bibtex.txt" class="btn-sm btn-primary">BibTeX</a>                                                
                        <br><br>
                        <div id="fairness-abstract" class="collapse" style="height: auto;">
                            Recent work on fairness in machine learning has primarily emphasized how to define, quantify, and encourage “fair” outcomes. Less attention has been paid, however, to the ethical foundations which underlie such efforts. Among the ethical perspectives that should be taken into consideration is consequentialism, the position that, roughly speaking, outcomes are all that matter. Although consequentialism is not free from difficulties, and although it does not necessarily provide a tractable way of choosing actions (because of the combined problems of uncertainty, subjectivity, and aggregation), it nevertheless provides a powerful foundation from which to critique the existing literature on machine learning fairness. Moreover, it brings to the fore some of the tradeoffs involved, including the problem of who counts, the pros and cons of using a policy, and the relative value of the distant future. In this paper we provide a consequentialist critique of common definitions of fairness within machine learning, as well as a machine learning perspective on consequentialism. We conclude with a broader discussion of the issues of learning and randomization, which have important implications for the ethics of automated decision making systems. 
                            <br><br>
                        </div>
                    </div>
                </div>
                <br>                
                <br>           

                <div class="row">
                <!-- <div class="col-md-1">&nbsp;</div> -->
                    <div class="col-md-4">
                        <img alt="Show Your Work Figure" src="resources/work.png" width="70%" style="float: center;"/>
                    </div>
                <!-- <div class="col-md-1">&nbsp;</div> -->
                    <div class="col-md-8"  style="background-color: #DCDCDC; margin: auto;">
                        <br><a target="_blank" href="https://arxiv.org/abs/1909.03004"><b>Show Your Work: Improved Reporting of Experimental Results</b></a><br>
                        Jesse Dodge, Suchin Gururangan, <u>Dallas Card</u>, Roy Schwartz, and Noah A. Smith<br>
                        In <i>Proceedings of EMNLP</i>, 2019.<br>
                        <a data-toggle="collapse" href="#emnlp2019-abstract" class="btn-sm btn-primary">Abstract</a>
                        <a target="_blank" href="https://arxiv.org/abs/1909.03004" class="btn-sm btn-primary">Paper</a>
                        <a target="_blank" href="https://github.com/allenai/show-your-work" class="btn-sm btn-primary">Code</a>
                        <a target="_blank" href="https://www.wired.com/story/artificial-intelligence-confronts-reproducibility-crisis/" class="btn-sm btn-primary">Press</a>
                        <a target="_blank" href="bibtex/emnlp2019-bibtex.txt" class="btn-sm btn-primary">BibTeX</a>                                                
                        <br><br>
                        <div id="emnlp2019-abstract" class="collapse" style="height: auto;">
                            Research in natural language processing proceeds, in part, by demonstrating that new models achieve superior performance (e.g., accuracy) on held-out test data, compared to previous results. In this paper, we demonstrate that test-set performance scores alone are insufficient for drawing accurate conclusions about which model performs best. We argue for reporting additional details, especially performance on validation data obtained during model development. We present a novel technique for doing so: expected validation performance of the best-found model as a function of computation budget (i.e., the number of hyperparameter search trials or the overall training time). Using our approach, we find multiple recent model comparisons where authors would have reached a different conclusion if they had used more (or less) computation. Our approach also allows us to estimate the amount of computation required to obtain a given accuracy; applying it to several recently published results yields massive variation across papers, from hours to weeks. We conclude with a set of best practices for reporting experimental results which allow for robust future comparisons, and provide code to allow researchers to use our technique.
                            <br><br>
                        </div>
                    </div>
                </div>
                <br>
                <br>
                <div class="row">
                <!-- <div class="col-md-1">&nbsp;</div> -->
                    <div class="col-md-4">
                        <img alt="VAMPIRE Figure" src="resources/vampire.png" width="65%" style="float: center;"/>
                    </div>
                <!-- <div class="col-md-1">&nbsp;</div> -->
                    <div class="col-md-8"  style="background-color: #DCDCDC; margin: auto;">
                        <br><a target="_blank" href="https://arxiv.org/abs/1906.02242"><b>Variational Pretraining for Semi-supervised Text Classification</b></a><br>
                        Suchin Gururangan, Tam Dang, <u>Dallas Card</u>, and Noah A. Smith<br>
                        In <i>Proceedings of ACL</i>, 2019.<br>
                        <a data-toggle="collapse" href="#vampire-abstract" class="btn-sm btn-primary">Abstract</a>
                        <a target="_blank" href="https://arxiv.org/abs/1906.02242" class="btn-sm btn-primary">Paper</a>
                        <a target="_blank" href="http://github.com/allenai/vampire" class="btn-sm btn-primary">Code</a>
                        <a target="_blank" href="bibtex/acl2019-bibtex.txt" class="btn-sm btn-primary">BibTeX</a>                                                
                        <br><br>
                        <div id="vampire-abstract" class="collapse" style="height: auto;">
                            We introduce VAMPIRE, a lightweight pretraining framework for effective text classification when data and computing resources are limited. We pretrain a unigram document model as a variational autoencoder on in-domain, unlabeled data and use its internal states as features in a downstream classifier. Empirically, we show the relative strength of VAMPIRE against computationally expensive contextual embeddings and other popular semi-supervised baselines under low resource settings. We also find that fine-tuning to in-domain data is crucial to achieving decent performance from contextual embeddings when working with limited supervision. We accompany this paper with code to pretrain and use VAMPIRE embeddings in downstream tasks.
                            <br><br>
                        </div>
                    </div>
                </div>
                <br>
                <br>
                <div class="row">
                <!-- <div class="col-md-1">&nbsp;</div> -->
                    <div class="col-md-4">
                        <img alt="Hatespeech Figure" src="resources/hatespeech.png" width="70%" style="float: center;"/>
                    </div>
                <!-- <div class="col-md-1">&nbsp;</div> -->
                    <div class="col-md-8"  style="background-color: #DCDCDC; margin: auto;">
                        <br><a target="_blank" href="resources/TwitterAAE_ACL.pdf"><b>The Risk of Racial Bias in Hate Speech Detection</b></a><br>
                        Maarten Sap, <u>Dallas Card</u>, Saadia Gabriel, Yejin Choi, and Noah A. Smith<br>
                        In <i>Proceedings of ACL</i>, 2019.<br>
                        <a data-toggle="collapse" href="#hatespeech-abstract" class="btn-sm btn-primary">Abstract</a>
                        <a target="_blank" href="https://aclanthology.org/P19-1163.pdf" class="btn-sm btn-primary">Paper</a>
                        <a target="_blank" href="https://www.vox.com/recode/2019/8/15/20806384/social-media-hate-speech-bias-black-african-american-facebook-twitter?" class="btn-sm btn-primary">Press</a>
                        <a target="_blank" href="bibtex/acl2019-hatespeech-bibtex.txt" class="btn-sm btn-primary">BibTeX</a>                                                
                        <br><br>
                        <div id="hatespeech-abstract" class="collapse" style="height: auto;">
                            We investigate how annotators' insensitivity to differences in dialect can lead to racial bias in automatic hate speech detection models, potentially amplifying harm against minority populations. We first uncover unexpected correlations between surface markers of African American English (AAE) and ratings of toxicity in several widely-used hate speech datasets. Then, we show that models trained on these corpora acquire and propagate these biases, such that AAE tweets and tweets by self-identified African Americans are up to two times more likely to be labelled as offensive compared to others. Finally, we propose dialect and race priming as ways to reduce the racial bias in annotation, showing that when annotators are made explicitly aware of an AAE tweet's dialect they are significantly less likely to label the tweet as offensive.
                            <br><br>
                        </div>
                    </div>
                </div>
                <br>
                <br>                
                <div class="row">
                <!-- <div class="col-md-1">&nbsp;</div> -->
                    <div class="col-md-4">
                        <img alt="DWAC Figure" src="resources/dwac.png" width="80%" style="float: center;"/>
                    </div>
                <!-- <div class="col-md-1">&nbsp;</div> -->
                    <div class="col-md-8"  style="background-color: #DCDCDC; margin: auto;">
                        <br><a target="_blank" href="https://arxiv.org/abs/1811.02579"><b>Deep Weighted Averaging Classifiers</b></a><br>
                        <u>Dallas Card</u>, Michael Zhang, and Noah A. Smith<br>
                        In <i>Proceedings of the ACM Conference on Fairness, Accountability, and Transparency (ACM FAT*)</i>, 2019.<br>
                        <a data-toggle="collapse" href="#dwac-abstract" class="btn-sm btn-primary">Abstract</a>
                        <a target="_blank" href="https://arxiv.org/abs/1811.02579" class="btn-sm btn-primary">Paper</a>
                        <a target="_blank" href="https://github.com/dallascard/DWAC"  class="btn-sm btn-primary">Code</a>
                        <a target="_blank" href="https://medium.com/@dallascard/deep-weighted-averaging-classifiers-7aa1bd763d4f"  class="btn-sm btn-primary">Blog Post</a>
                        <a target="_blank" href="bibtex/fat2019-bibtex.txt" class="btn-sm btn-primary">BibTeX</a>
                        <br><br>
                        <div id="dwac-abstract" class="collapse" style="height: auto;">
                            Recent advances in deep learning have achieved impressive gains in classification accuracy on a variety of types of data, including images and text. Despite these gains, however, concerns have been raised about the calibration, robustness, and interpretability of these models. In this paper we propose a simple way to modify any conventional deep architecture to automatically provide more transparent explanations for classification decisions, as well as an intuitive notion of the credibility of each prediction. Specifically, we draw on ideas from nonparametric kernel regression, and propose to predict labels based on a weighted sum of training instances, where the weights are determined by distance in a learned instance-embedding space. Working within the framework of conformal methods, we propose a new measure of nonconformity suggested by our model, and experimentally validate the accompanying theoretical expectations, demonstrating improved transparency, controlled error rates, and robustness to out-of-domain data, without compromising on accuracy or calibration.
                            <br><br>
                        </div>
                    </div>
                </div>
                <br>
                <br>
                <div class="row">
                    <div class="col-md-4">
                        <img alt="Scholar Figure" src="resources/scholar.png" width="70%"/>
                    </div>
                    <div class="col-md-8"  style="background-color: #DCDCDC; margin: auto;">
                        <br><a target="_blank" href="https://arxiv.org/abs/1705.09296"><b>Neural Models for Documents with Metadata</b></a><br>
                        <u>Dallas Card</u>, Chenhao Tan, and Noah A. Smith<br>
                        In <i>Proceedings of ACL</i>, 2018.<br>
                        <a data-toggle="collapse" href="#scholar-abstract" class="btn-sm btn-primary">Abstract</a>
                        <a target="_blank" href="https://arxiv.org/abs/1705.09296"  class="btn-sm btn-primary">Paper</a>
                        <a target="_blank" href="https://github.com/dallascard/scholar"  class="btn-sm btn-primary">Code</a>
                        <a target="_blank" href="https://github.com/dallascard/scholar/blob/master/tutorial.ipynb"  class="btn-sm btn-primary">Tutorial</a>
                        <a target="_blank" href="bibtex/acl2018-bibtex.txt" class="btn-sm btn-primary">BibTeX</a>                                                
                        <br><br>
                        <div id="scholar-abstract" class="collapse">
                            Most real-world document collections involve various types of metadata, such as author, source, and date, and yet the most commonly-used approaches to modeling text corpora ignore this information. While specialized models have been developed for particular applications, few are widely used in practice, as customization typically requires derivation of a custom inference algorithm. In this paper, we build on recent advances in variational inference methods and propose a general neural framework, based on topic models, to enable flexible incorporation of metadata and allow for rapid exploration of alternative models. Our approach achieves strong performance, with a manageable tradeoff between perplexity, coherence, and sparsity. Finally, we demonstrate the potential of our framework through an exploration of a corpus of articles about US immigration.
                            <br><br>
                        </div>
                    </div>
                </div>
                <br>
                <br>
                <div class="row">
                    <div class="col-md-4">
                        <img alt="Proportions Figure" src="resources/proportions.png" width="80%"/>
                    </div>
                    <div class="col-md-8"  style="background-color: #DCDCDC; margin: auto;">
                        <br><a target="_blank" href="resources/naacl2018.pdf"><b>The Importance of Calibration for Estimating Proportions from Annotations</b></a><br>
                        <u>Dallas Card</u>, and Noah A. Smith<br>
                        In <i>Proceedings of NAACL</i>, 2018.<br>
                        <a data-toggle="collapse" href="#proportions-abstract" class="btn-sm btn-primary">Abstract</a>
                        <a target="_blank" target="_blank" href="https://aclanthology.org/N18-1148.pdf" class="btn-sm btn-primary">Paper</a>
                        <a target="_blank" target="_blank" href="https://github.com/dallascard/proportions" class="btn-sm btn-primary">Code</a>
                        <a target="_blank" href="bibtex/naacl2018-bibtex.txt" class="btn-sm btn-primary">BibTeX</a>
                        <br><br>
                        <div id="proportions-abstract" class="collapse">
                            Estimating label proportions in a target corpus is a type of measurement that is useful for answering certain types of social-scientific questions. While past work has described a number of relevant approaches, nearly all are based on an assumption which we argue is invalid for many problems, particularly when dealing with human annotations. In this paper, we identify and differentiate between two relevant data generating scenarios (intrinsic vs. extrinsic labels), introduce a simple but novel method which emphasizes the importance of calibration, and then analyze and experimentally validate the appropriateness of various methods for each of the two scenarios.
                            <br><br>
                        </div>
                    </div>
                </div>
                <br>
                <br>
                <div class="row">
                    <div class="col-md-4">
                        <img alt="Ideas Figure" src="resources/ideas.png" width="80%"/>
                    </div>
                    <div class="col-md-8"  style="background-color: #DCDCDC; margin: auto;">
                        <br><a target="_blank" href="https://arxiv.org/abs/1704.07828"><b>Friendships, Rivalries, and Trysts: Characterizing Relations between Ideas in Texts</b></a><br>
                        Chenhao Tan, <u>Dallas Card</u>, and Noah A. Smith<br>
                        In <i>Proceedings of ACL</i>, 2017.<br>
                        <a data-toggle="collapse" href="#ideas-abstract" class="btn-sm btn-primary">Abstract</a>
                        <a target="_blank" href="https://arxiv.org/abs/1704.07828" class="btn-sm btn-primary">Paper</a>
                        <a target="_blank" href="https://medium.com/@ChenhaoTan/exploring-the-friendships-rivalries-and-trysts-between-ideas-in-texts-e9fc3f5a1253" class="btn-sm btn-primary">Blog Post</a>
                        <a target="_blank" href="bibtex/acl2017-bibtex.txt" class="btn-sm btn-primary">BibTeX</a>
                        <br><br>
                        <div id="ideas-abstract" class="collapse">
                            Understanding how ideas relate to each other is a fundamental question in many domains, ranging from intellectual history to public communication. Because ideas are naturally embedded in texts, we propose the first framework to systematically characterize the relations between ideas based on their occurrence in a corpus of documents, independent of how these ideas are represented. Combining two statistics - cooccurrence within documents and prevalence correlation over time - our approach reveals a number of different ways in which ideas can cooperate and compete. For instance, two ideas can closely track each other's prevalence over time, and yet rarely cooccur, almost like a "cold war" scenario. We observe that pairwise cooccurrence and prevalence correlation exhibit different distributions. We further demonstrate that our approach is able to uncover intriguing relations between ideas through in-depth case studies on news articles and research papers.
                            <br><br>
                        </div>
                    </div>
                </div>
                <br>
                <br>
                <div class="row">
                    <div class="col-md-4">
                        <img alt="Personas Figure" class="center" src="resources/personas.png" width="80%"/>
                    </div>
                    <div class="col-md-8"  style="background-color: #DCDCDC; margin: auto;">
                        <br><a target="_blank" href="resources/emnlp2016.pdf"><b>Analyzing Framing through the Casts of Characters in the News</b></a><br>
                        <u>Dallas Card</u>, Justin H. Gross, Amber E. Boydstun, and Noah A. Smith<br>
                        In <i>Proceedings of EMNLP</i>, 2016.<br>
                        <a data-toggle="collapse" href="#personas-abstract" class="btn-sm btn-primary">Abstract</a>
                        <a target="_blank" href="https://aclanthology.org/D16-1148.pdf"  class="btn-sm btn-primary">Paper</a>
                        <a target="_blank" href="bibtex/emnlp2016-bibtex.txt" class="btn-sm btn-primary">BibTeX</a>                        
                        <br><br>
                        <div id="personas-abstract" class="collapse">
                            We present an unsupervised model for the discovery and clustering of latent "personas" (characterizations of entities). Our model simultaneously clusters documents featuring similar collections of personas. We evaluate this model on a collection of news articles about immigration, showing that personas help predict the coarse-grained framing annotations in the Media Frames Corpus. We also introduce automated model selection as a fair and robust form of feature evaluation.
                            <br><br>
                        </div>
                    </div>
                </div>
                <br>
                <br>
                <div class="row">
                    <div class="col-md-4">
                        <img alt="Media Frames Corpus Figure" src="resources/mfc.png" width="80%"/>
                    </div>
                    <div class="col-md-8"  style="background-color: #DCDCDC; margin: auto;">
                        <br><a target="_blank" href="resources/card.acl2015.pdf"><b>The Media Frames Corpus: Annotations of Frames Across Issues</b></a><br>
                        <u>Dallas Card</u>, Amber E. Boydstun, Justin H. Gross, Philip Resnik, and Noah A. Smith<br>
                        In <i>Proceedings of ACL</i>, 2015.<br>
                        <a data-toggle="collapse" href="#mfc-abstract" class="btn-sm btn-primary">Abstract</a>
                        <a target="_blank" href="https://aclanthology.org/P15-2072.pdf"  class="btn-sm btn-primary">Paper</a>
                        <a target="_blank" href="https://github.com/dallascard/media_frames_corpus" class="btn-sm btn-primary">Data</a>
                        <a target="_blank" href="bibtex/acl2015-bibtex.txt" class="btn-sm btn-primary">BibTeX</a>
                        <br><br>
                        <div id="mfc-abstract" class="collapse">
                                We describe the first version of the Media Frames Corpus: several thousand news articles on three policy issues, annotated in terms of media framing. We motivate framing as a phenomenon of study for computational linguistics and describe our annotation process.
                            <br><br>
                        </div>
                    </div>
                </div>
                <br><br>
                <div class="row">
                    <h2> Media Coverage </h2>
                </div>
                    <ul>
                        <li>"Study finds that few major AI research papers consider negative impacts" by Kyle Wiggers. <a target="_black" href="https://venturebeat.com/2021/07/01/study-finds-that-few-major-ai-research-papers-consider-negative-impacts/">VentureBeat</a> (2021).</li>
                        <li>"Artificial Intelligence Confronts a 'Reproducibility' Crisis" by Gregory Barber. <a target="_black" href="https://www.wired.com/story/artificial-intelligence-confronts-reproducibility-crisis">WIRED</a> (2019).</li>
                        <li>"The algorithms that detect hate speech online are biased against black people" by Shirin Ghaffary. <a target="_black" href="https://www.vox.com/recode/2019/8/15/20806384/social-media-hate-speech-bias-black-african-american-facebook-twitter">Vox</a> (2019).</li>         
                    </ul>                
                <br>
                <div class="row">
                    <h2> About me </h2>
                </div>
                <br>                
                <div class="row">                    
                    <div class="col-md-12">
                        I'm originally from <a target="_blank" href="https://goo.gl/maps/rZmFkqGnSwf6tsfw9">Winnipeg</a>, but I have also lived in <a target="_blank" href="https://goo.gl/maps/VeT7w7auj3KfmhBR9">Toronto</a>, <a target="_blank" href="https://goo.gl/maps/dcr6Yy4zwapHnHB3A">Waterloo</a>, <a target="_blank" href="https://goo.gl/maps/ZUFHQdmizsmWVsRM9">Halifax</a>, <a target="_blank" href="https://goo.gl/maps/X4K2T7j44GU8g1qk9">Sydney</a>, <a target="_blank" href="https://goo.gl/maps/Umqw3k9YmiUqrxcg7">Kampala</a>, <a target="_blank" href="https://goo.gl/maps/tjEKUzr3UwHE29ug7">Pittsburgh</a>, <a target="_blank" href="https://goo.gl/maps/eWdKTPcdhALCFC5R8">Seattle</a>, <a target="_blank" href="https://goo.gl/maps/geFZfNXwh3HbEJS79">Palo Alto</a>, and now <a target="_blank" href="https://goo.gl/maps/712amukiU3UbPzfQ9">Ann Arbor</a>!
                          
                    </div>
                </div>
                <br>
                <div class="row">                
                    <div class="col-md-12">
                        I am an occasional guest on <a target="_blank" href="http://www.trcpodcast.com">The Reality Check</a> podcast! You can hear me in episodes <a target="_blank" href="http://www.trcpodcast.com/trc-466-science-of-summer-biased-algorithms-lord-of-the-flies-plausibility/">#466</a> (biased algorithms), <a target="_blank" href="http://www.trcpodcast.com/trc-382-dunning-kruger-effect-deep-learning-do-the-british-have-bad-teeth/">#382</a> (deep learning), <a target="_blank" href="http://www.trcpodcast.com/trc-362-simpsons-paradox-name-that-gmo-edition-travellers-diarrhea-myths/">#362</a> (Simpson's paradox), and <a target="_blank" href="http://www.trcpodcast.com/trc-227-fmri-high-heels-sally-field/">#227</a> (fMRI and vegetative states).
                    </div>
                </div>
                <br>
                <div class="row">
                    <div class="col-md-12">
                        [<a target="_blank" href="resources/short_bio.txt">short bio for talks</a>]
                    </div>
                </div>                

               
                <br><br>
                <div class="icons">
                    <a target="_blank" href="https://github.com/dallascard"><img src="resources/github.png" width=32 alt="GitHub Icon"></a>
                    <a target="_blank" href="https://twitter.com/dallascard"><img src="resources/twitter.png" width=32 alt="Twitter Icon"></a>
                    <!-- <a target="_blank" href="https://medium.com/@dallascard"><img src="resources/medium.png" width=32 alt="Medium Icon"></a> -->
                    <a target="_blank" href="https://scholar.google.com/citations?hl=en&user=qH-rJV8AAAAJ&view_op=list_works&sortby=pubdate"><img src="resources/gscholar.png" width=32 alt="Google Scholar Icon"></a>
                </div>

            </div>
        </div>
